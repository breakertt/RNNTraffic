{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNTraffic\n",
    "\n",
    "**RNNTraffic** is a Recurrent Neural Network based network traffic classification implementation. In this notebook, the whole process of data cleaning, training and testing will be explanied.\n",
    "\n",
    "This implementation has two main features:\n",
    "1. Classify where a network traffic in under OpenVPN.\n",
    "2. Classify detailed type of the traffic.\n",
    "\n",
    "And there are mainly three parts in this implementation:\n",
    "1. Data prepare and process\n",
    "2. Model tranining and tuning\n",
    "3. Final result evaluation\n",
    "\n",
    "## 1 Data prepare and process\n",
    "\n",
    "For the first part, code is not include in this notebook. The detail will explanined here.\n",
    "\n",
    "### 1.1 Dataset selection\n",
    "\n",
    "The ISCXVPN2016 dataset which contains various types of traffic including regular traffic, TLS encryted traffic and OpenVPN encryted traffic of some common applications and services. And there are also already bunch of related work on this dataset. Therefore, [ISCXVPN2016](http://205.174.165.80/CICDataset/ISCX-VPN-NonVPN-2016/Dataset/) is used for `RNNTraffic`.\n",
    "\n",
    "### 1.2 Data selection\n",
    "\n",
    "Due to the time limitation of this final project and our hardware computing power. It is not pratical to modeling all the data offered by ISCXVPN2016.\n",
    "\n",
    "Among the various traffic types, four each from VPN and non-VPN was selected.\n",
    "The selected ones are \"Chat\", \"Email\", \"P2P\" and \"Streaming\". These four are typical modern network traffic.\n",
    "\n",
    "### 1.3 Split `pcap` files to session\n",
    "\n",
    "There, large `pcap` files are splitted by `SplitCap` tool to each session. It is kind of obvious that session contains better traffic fingerprint than single flow(packet).\n",
    "\n",
    "Due to `SplitCap` not supporting `.pcapng`, all files are converted `.pcap` first. Secnondly, a `powershell` script can be called for splitting all `.pacp` files. The usage of script is `.\\datasets_processed\\packet2seesion.ps1` under PowerShell.\n",
    "\n",
    "In this script, each `.pacp` file in `\".\\datasets_selected\\\"` are splitted to sessions and saved in `\".\\datasets_processed\\\"` recursively.\n",
    "\n",
    "### 1.4 Convert sesssions to CSV files\n",
    "\n",
    "Now there are many `.pcap` files which represents sessions. It essential to get more sufficient data from there files. To solve this problem, a naive approach is applied for `RNNTraffic`.\n",
    "\n",
    "The conversion is done by the script `.datasets_processed\\session2csv.py\\`.\n",
    "\n",
    "In this script, `.pcap` file of session is read as raw unsigned int8 datas to numpy array. Sessions with more than 1500 bytes are trimed, session with less than 300 bytes are deprecated and other session are repeated to fill 1500 bytes.\n",
    "\n",
    "Each session is stored as one row in every dataframe, and the dataframe is stored as CSV files for future trainning and testing.\n",
    "\n",
    "## 2 Model tranining and tuning\n",
    "\n",
    "### 2.0 Import packages and test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyTorch version: 1.4.0\n",
      "GPU Avaliability: True\n",
      "Current working directory: /home/tygao/py-repos/RNNTraffic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"pyTorch version: {}\".format(torch.__version__))\n",
    "print(\"GPU Avaliability: {}\".format(torch.cuda.is_available()))\n",
    "print(\"Current working directory: {}\".format(os.getcwd()))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of VPN dataframe: (3984, 1501), shape of non-VPN dataframe: (3429, 1501)\n",
      "Vpn labels: ['VPN-Email', 'VPN-P2P', 'VPN-Chat', 'VPN-Streaming'], non-VPN labels: ['P2P', 'Chat', 'Streaming', 'Email']\n"
     ]
    }
   ],
   "source": [
    "vpnDatasetDir = \"./datasets_processed/VPN\"\n",
    "nonVpnDatasetDir = \"./datasets_processed/non-VPN\"\n",
    "\n",
    "def getCsvFiles(rootdir):\n",
    "    csvFiles = []\n",
    "    for root, subdirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == \".csv\":\n",
    "                csvFiles.append(root + os.path.sep + file)\n",
    "    return csvFiles\n",
    "\n",
    "def readCsvFilesToDataframes(csvFiles):\n",
    "    dfList = []\n",
    "    for csvFile in csvFiles:\n",
    "        df = pd.read_csv(csvFile, index_col=0)\n",
    "        dfList.append(df)\n",
    "    return dfList\n",
    "\n",
    "vpnCsvFiles = getCsvFiles(vpnDatasetDir)\n",
    "vpnDataframes = readCsvFilesToDataframes(vpnCsvFiles)\n",
    "\n",
    "# Add indication on label for VPN data\n",
    "for vpnDataframe in vpnDataframes:\n",
    "    vpnDataframe['Label'] = \"VPN-\" + vpnDataframe['Label']\n",
    "    \n",
    "nonVpnCsvFiles = getCsvFiles(nonVpnDatasetDir)\n",
    "nonVpnDataframes = readCsvFilesToDataframes(nonVpnCsvFiles)\n",
    "\n",
    "vpnDf = pd.concat(vpnDataframes, ignore_index=True, sort=False)\n",
    "nonVpnDf = pd.concat(nonVpnDataframes, ignore_index=True, sort=False)\n",
    "\n",
    "print(\"Shape of VPN dataframe: {}, shape of non-VPN dataframe: {}\".format(vpnDf.shape, nonVpnDf.shape))\n",
    "\n",
    "vpnLabels = list(set(vpnDf['Label'].to_list()))\n",
    "nonVpnLabels = list(set(nonVpnDf['Label'].to_list()))\n",
    "\n",
    "print(\"Vpn labels: {}, non-VPN labels: {}\".format(vpnLabels, nonVpnLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataIndex_0</th>\n",
       "      <th>DataIndex_1</th>\n",
       "      <th>DataIndex_2</th>\n",
       "      <th>DataIndex_3</th>\n",
       "      <th>DataIndex_4</th>\n",
       "      <th>DataIndex_5</th>\n",
       "      <th>DataIndex_6</th>\n",
       "      <th>DataIndex_7</th>\n",
       "      <th>DataIndex_8</th>\n",
       "      <th>DataIndex_9</th>\n",
       "      <th>...</th>\n",
       "      <th>DataIndex_1491</th>\n",
       "      <th>DataIndex_1492</th>\n",
       "      <th>DataIndex_1493</th>\n",
       "      <th>DataIndex_1494</th>\n",
       "      <th>DataIndex_1495</th>\n",
       "      <th>DataIndex_1496</th>\n",
       "      <th>DataIndex_1497</th>\n",
       "      <th>DataIndex_1498</th>\n",
       "      <th>DataIndex_1499</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>221</td>\n",
       "      <td>61</td>\n",
       "      <td>204</td>\n",
       "      <td>163</td>\n",
       "      <td>237</td>\n",
       "      <td>187</td>\n",
       "      <td>254</td>\n",
       "      <td>VPN-Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>177</td>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "      <td>248</td>\n",
       "      <td>223</td>\n",
       "      <td>123</td>\n",
       "      <td>142</td>\n",
       "      <td>145</td>\n",
       "      <td>176</td>\n",
       "      <td>VPN-Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VPN-Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>225</td>\n",
       "      <td>209</td>\n",
       "      <td>124</td>\n",
       "      <td>232</td>\n",
       "      <td>156</td>\n",
       "      <td>108</td>\n",
       "      <td>65</td>\n",
       "      <td>VPN-Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>82</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>207</td>\n",
       "      <td>204</td>\n",
       "      <td>157</td>\n",
       "      <td>VPN-Chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DataIndex_0  DataIndex_1  DataIndex_2  DataIndex_3  DataIndex_4  \\\n",
       "0          161          178          195          212            0   \n",
       "1          161          178          195          212            0   \n",
       "2          161          178          195          212            0   \n",
       "3          161          178          195          212            0   \n",
       "4          161          178          195          212            0   \n",
       "\n",
       "   DataIndex_5  DataIndex_6  DataIndex_7  DataIndex_8  DataIndex_9  ...  \\\n",
       "0            2            0            4            0            0  ...   \n",
       "1            2            0            4            0            0  ...   \n",
       "2            2            0            4            0            0  ...   \n",
       "3            2            0            4            0            0  ...   \n",
       "4            2            0            4            0            0  ...   \n",
       "\n",
       "   DataIndex_1491  DataIndex_1492  DataIndex_1493  DataIndex_1494  \\\n",
       "0              38              12             221              61   \n",
       "1             177             137               7             248   \n",
       "2             214             131               0               0   \n",
       "3              14              16             225             209   \n",
       "4              70              82              64               0   \n",
       "\n",
       "   DataIndex_1495  DataIndex_1496  DataIndex_1497  DataIndex_1498  \\\n",
       "0             204             163             237             187   \n",
       "1             223             123             142             145   \n",
       "2               0              52               0               0   \n",
       "3             124             232             156             108   \n",
       "4              50              17             207             204   \n",
       "\n",
       "   DataIndex_1499     Label  \n",
       "0             254  VPN-Chat  \n",
       "1             176  VPN-Chat  \n",
       "2               0  VPN-Chat  \n",
       "3              65  VPN-Chat  \n",
       "4             157  VPN-Chat  \n",
       "\n",
       "[5 rows x 1501 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpnDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataIndex_0</th>\n",
       "      <th>DataIndex_1</th>\n",
       "      <th>DataIndex_2</th>\n",
       "      <th>DataIndex_3</th>\n",
       "      <th>DataIndex_4</th>\n",
       "      <th>DataIndex_5</th>\n",
       "      <th>DataIndex_6</th>\n",
       "      <th>DataIndex_7</th>\n",
       "      <th>DataIndex_8</th>\n",
       "      <th>DataIndex_9</th>\n",
       "      <th>...</th>\n",
       "      <th>DataIndex_1491</th>\n",
       "      <th>DataIndex_1492</th>\n",
       "      <th>DataIndex_1493</th>\n",
       "      <th>DataIndex_1494</th>\n",
       "      <th>DataIndex_1495</th>\n",
       "      <th>DataIndex_1496</th>\n",
       "      <th>DataIndex_1497</th>\n",
       "      <th>DataIndex_1498</th>\n",
       "      <th>DataIndex_1499</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>231</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>101</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>103</td>\n",
       "      <td>166</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>159</td>\n",
       "      <td>46</td>\n",
       "      <td>182</td>\n",
       "      <td>Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>109</td>\n",
       "      <td>186</td>\n",
       "      <td>136</td>\n",
       "      <td>183</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>Chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DataIndex_0  DataIndex_1  DataIndex_2  DataIndex_3  DataIndex_4  \\\n",
       "0          161          178          195          212            0   \n",
       "1          161          178          195          212            0   \n",
       "2          161          178          195          212            0   \n",
       "3          161          178          195          212            0   \n",
       "4          161          178          195          212            0   \n",
       "\n",
       "   DataIndex_5  DataIndex_6  DataIndex_7  DataIndex_8  DataIndex_9  ...  \\\n",
       "0            2            0            4            0            0  ...   \n",
       "1            2            0            4            0            0  ...   \n",
       "2            2            0            4            0            0  ...   \n",
       "3            2            0            4            0            0  ...   \n",
       "4            2            0            4            0            0  ...   \n",
       "\n",
       "   DataIndex_1491  DataIndex_1492  DataIndex_1493  DataIndex_1494  \\\n",
       "0             231              50              60             101   \n",
       "1              17             103             166              34   \n",
       "2              84             109             186             136   \n",
       "3               1               0               0               0   \n",
       "4               4               5             180               1   \n",
       "\n",
       "   DataIndex_1495  DataIndex_1496  DataIndex_1497  DataIndex_1498  \\\n",
       "0              80               8               0              69   \n",
       "1               1             187             159              46   \n",
       "2             183              38               0              38   \n",
       "3               0               0               0              32   \n",
       "4               3               3               8               1   \n",
       "\n",
       "   DataIndex_1499  Label  \n",
       "0               0   Chat  \n",
       "1             182   Chat  \n",
       "2               8   Chat  \n",
       "3              70   Chat  \n",
       "4               1   Chat  \n",
       "\n",
       "[5 rows x 1501 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonVpnDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 VPN/non-VPN binary classification - dataloader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonVpnDf['Label'] = \"0\"\n",
    "nonVpnDf['Label'] = nonVpnDf['Label'].astype('int64')\n",
    "vpnDf['Label'] = \"1\"\n",
    "vpnDf['Label'] = vpnDf['Label'].astype('int64')\n",
    "df = pd.concat([nonVpnDf, vpnDf], ignore_index=True, sort=False)\n",
    "trainDf, validateDf, testDf = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5930, 1501), (741, 1501), (742, 1501))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.shape, validateDf.shape, testDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VpnBinaryDataset():\n",
    "\n",
    "    def __init__(self, dataframe, transform=None, target_transform = None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = np.array([self.df.iloc[index, :-1]])\n",
    "        data = torch.from_numpy(data).view(1, df.shape[1] - 1).float()\n",
    "        target = self.df.iloc[index, -1]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1500]),\n",
       " torch.float32,\n",
       " tensor([[161., 178., 195.,  ..., 178., 195., 212.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset = VpnBinaryDataset(trainDf)\n",
    "validateDataset = VpnBinaryDataset(validateDf)\n",
    "testDataset = VpnBinaryDataset(testDf)\n",
    "trainDataset[0][0].shape, trainDataset[0][0].dtype, trainDataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaderArgs = {'batch_size': 1000}\n",
    "\n",
    "if use_cuda:\n",
    "    loaderArgs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "                     )\n",
    "else:\n",
    "    loaderArgs.update({'shuffle': True})\n",
    "    \n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset,**loaderArgs)\n",
    "validateLoader = torch.utils.data.DataLoader(validateDataset, **loaderArgs)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, **loaderArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define train, validate and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args['dry-run']:\n",
    "                break\n",
    "                \n",
    "def validate(model, device, validate_loader):\n",
    "    model.eval()\n",
    "    validate_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in validate_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validate_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            preds.append(pred.cpu())\n",
    "            targets.append(target.view_as(pred).cpu())\n",
    "\n",
    "    pred_all = torch.cat(preds).squeeze().tolist()\n",
    "    target_all = torch.cat(targets).squeeze().tolist()\n",
    "            \n",
    "    rc = recall_score(pred_all, target_all, average='macro')\n",
    "    pr = precision_score(pred_all, target_all, average='macro')\n",
    "    \n",
    "    validate_loss /= len(validate_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(validate_loader.dataset)\n",
    "    \n",
    "    print('\\nValidate set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n, Recall: {}, Precision: {}\\n'.format(\n",
    "        validate_loss, correct, len(validate_loader.dataset), accuracy, rc, pr))\n",
    "\n",
    "    return validate_loader, accuracy\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            preds.append(pred.cpu())\n",
    "            targets.append(target.view_as(pred).cpu())\n",
    "\n",
    "    pred_all = torch.cat(preds).squeeze().tolist()\n",
    "    target_all = torch.cat(targets).squeeze().tolist()\n",
    "            \n",
    "    rc = recall_score(pred_all, target_all, average='macro')\n",
    "    pr = precision_score(pred_all, target_all, average='macro')\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n, Recall: {}, Precision: {}\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy, rc, pr))\n",
    "\n",
    "    return test_loader, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 VPN/non-VPN binary classification - CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBinary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBinary, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, 7, 1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 7, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(11840, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st Conv\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        # 2nd Conv\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        # Full Connect\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5930 (0%)]\tLoss: 0.945723\n",
      "Train Epoch: 1 [2000/5930 (33%)]\tLoss: 13.238525\n",
      "Train Epoch: 1 [4000/5930 (67%)]\tLoss: 2.075236\n",
      "\n",
      "Validate set: Average loss: 3.7196, Accuracy: 400/741 (54%)\n",
      ", Recall: 0.2699055330634278, Precision: 0.5\n",
      "\n",
      "Train Epoch: 2 [0/5930 (0%)]\tLoss: 3.830102\n",
      "Train Epoch: 2 [2000/5930 (33%)]\tLoss: 2.560807\n",
      "Train Epoch: 2 [4000/5930 (67%)]\tLoss: 2.506182\n",
      "\n",
      "Validate set: Average loss: 2.6410, Accuracy: 400/741 (54%)\n",
      ", Recall: 0.2699055330634278, Precision: 0.5\n",
      "\n",
      "Train Epoch: 3 [0/5930 (0%)]\tLoss: 2.756702\n",
      "Train Epoch: 3 [2000/5930 (33%)]\tLoss: 0.642864\n",
      "Train Epoch: 3 [4000/5930 (67%)]\tLoss: 0.533678\n",
      "\n",
      "Validate set: Average loss: 0.3153, Accuracy: 633/741 (85%)\n",
      ", Recall: 0.8937007874015748, Precision: 0.8416422287390029\n",
      "\n",
      "Train Epoch: 4 [0/5930 (0%)]\tLoss: 0.296568\n",
      "Train Epoch: 4 [2000/5930 (33%)]\tLoss: 0.218709\n",
      "Train Epoch: 4 [4000/5930 (67%)]\tLoss: 0.208189\n",
      "\n",
      "Validate set: Average loss: 0.2052, Accuracy: 707/741 (95%)\n",
      ", Recall: 0.9533933518005541, Precision: 0.9559860703812317\n",
      "\n",
      "Train Epoch: 5 [0/5930 (0%)]\tLoss: 0.199642\n",
      "Train Epoch: 5 [2000/5930 (33%)]\tLoss: 0.182898\n",
      "Train Epoch: 5 [4000/5930 (67%)]\tLoss: 0.186206\n",
      "\n",
      "Validate set: Average loss: 0.1884, Accuracy: 707/741 (95%)\n",
      ", Recall: 0.9537804718122076, Precision: 0.9566348973607038\n",
      "\n",
      "Train Epoch: 6 [0/5930 (0%)]\tLoss: 0.186446\n",
      "Train Epoch: 6 [2000/5930 (33%)]\tLoss: 0.173750\n",
      "Train Epoch: 6 [4000/5930 (67%)]\tLoss: 0.162242\n",
      "\n",
      "Validate set: Average loss: 0.1749, Accuracy: 709/741 (96%)\n",
      ", Recall: 0.9563246866802682, Precision: 0.9591348973607039\n",
      "\n",
      "Train Epoch: 7 [0/5930 (0%)]\tLoss: 0.157945\n",
      "Train Epoch: 7 [2000/5930 (33%)]\tLoss: 0.164245\n",
      "Train Epoch: 7 [4000/5930 (67%)]\tLoss: 0.168016\n",
      "\n",
      "Validate set: Average loss: 0.1646, Accuracy: 709/741 (96%)\n",
      ", Recall: 0.9563246866802682, Precision: 0.9591348973607039\n",
      "\n",
      "Train Epoch: 8 [0/5930 (0%)]\tLoss: 0.156844\n",
      "Train Epoch: 8 [2000/5930 (33%)]\tLoss: 0.149621\n",
      "Train Epoch: 8 [4000/5930 (67%)]\tLoss: 0.157019\n",
      "\n",
      "Validate set: Average loss: 0.1566, Accuracy: 709/741 (96%)\n",
      ", Recall: 0.9563246866802682, Precision: 0.9591348973607039\n",
      "\n",
      "Train Epoch: 9 [0/5930 (0%)]\tLoss: 0.147973\n",
      "Train Epoch: 9 [2000/5930 (33%)]\tLoss: 0.142745\n",
      "Train Epoch: 9 [4000/5930 (67%)]\tLoss: 0.145767\n",
      "\n",
      "Validate set: Average loss: 0.1497, Accuracy: 716/741 (97%)\n",
      ", Recall: 0.9654484589465699, Precision: 0.9678848973607038\n",
      "\n",
      "Train Epoch: 10 [0/5930 (0%)]\tLoss: 0.144616\n",
      "Train Epoch: 10 [2000/5930 (33%)]\tLoss: 0.131982\n",
      "Train Epoch: 10 [4000/5930 (67%)]\tLoss: 0.142333\n",
      "\n",
      "Validate set: Average loss: 0.1446, Accuracy: 717/741 (97%)\n",
      ", Recall: 0.9668253875658096, Precision: 0.9693511730205279\n",
      "\n",
      "Train Epoch: 11 [0/5930 (0%)]\tLoss: 0.142037\n",
      "Train Epoch: 11 [2000/5930 (33%)]\tLoss: 0.137285\n",
      "Train Epoch: 11 [4000/5930 (67%)]\tLoss: 0.132947\n",
      "\n",
      "Validate set: Average loss: 0.1405, Accuracy: 718/741 (97%)\n",
      ", Recall: 0.9681505900199834, Precision: 0.9706011730205278\n",
      "\n",
      "Train Epoch: 12 [0/5930 (0%)]\tLoss: 0.127833\n",
      "Train Epoch: 12 [2000/5930 (33%)]\tLoss: 0.126564\n",
      "Train Epoch: 12 [4000/5930 (67%)]\tLoss: 0.134385\n",
      "\n",
      "Validate set: Average loss: 0.1370, Accuracy: 719/741 (97%)\n",
      ", Recall: 0.9694831057422969, Precision: 0.9718511730205279\n",
      "\n",
      "Train Epoch: 13 [0/5930 (0%)]\tLoss: 0.124943\n",
      "Train Epoch: 13 [2000/5930 (33%)]\tLoss: 0.125800\n",
      "Train Epoch: 13 [4000/5930 (67%)]\tLoss: 0.129084\n",
      "\n",
      "Validate set: Average loss: 0.1346, Accuracy: 719/741 (97%)\n",
      ", Recall: 0.969527045749537, Precision: 0.9720674486803519\n",
      "\n",
      "Train Epoch: 14 [0/5930 (0%)]\tLoss: 0.123533\n",
      "Train Epoch: 14 [2000/5930 (33%)]\tLoss: 0.126853\n",
      "Train Epoch: 14 [4000/5930 (67%)]\tLoss: 0.112350\n",
      "\n",
      "Validate set: Average loss: 0.1323, Accuracy: 719/741 (97%)\n",
      ", Recall: 0.9694831057422969, Precision: 0.9718511730205279\n",
      "\n",
      "Train Epoch: 15 [0/5930 (0%)]\tLoss: 0.123634\n",
      "Train Epoch: 15 [2000/5930 (33%)]\tLoss: 0.122957\n",
      "Train Epoch: 15 [4000/5930 (67%)]\tLoss: 0.114855\n",
      "\n",
      "Validate set: Average loss: 0.1306, Accuracy: 720/741 (97%)\n",
      ", Recall: 0.9708229972274915, Precision: 0.9731011730205279\n",
      "\n",
      "Train Epoch: 16 [0/5930 (0%)]\tLoss: 0.123847\n",
      "Train Epoch: 16 [2000/5930 (33%)]\tLoss: 0.115125\n",
      "Train Epoch: 16 [4000/5930 (67%)]\tLoss: 0.118190\n",
      "\n",
      "Validate set: Average loss: 0.1294, Accuracy: 721/741 (97%)\n",
      ", Recall: 0.9721857492997199, Precision: 0.9745674486803519\n",
      "\n",
      "Train Epoch: 17 [0/5930 (0%)]\tLoss: 0.123741\n",
      "Train Epoch: 17 [2000/5930 (33%)]\tLoss: 0.120696\n",
      "Train Epoch: 17 [4000/5930 (67%)]\tLoss: 0.119275\n",
      "\n",
      "Validate set: Average loss: 0.1285, Accuracy: 720/741 (97%)\n",
      ", Recall: 0.9708527210933967, Precision: 0.973317448680352\n",
      "\n",
      "Train Epoch: 18 [0/5930 (0%)]\tLoss: 0.112666\n",
      "Train Epoch: 18 [2000/5930 (33%)]\tLoss: 0.124869\n",
      "Train Epoch: 18 [4000/5930 (67%)]\tLoss: 0.113008\n",
      "\n",
      "Validate set: Average loss: 0.1277, Accuracy: 721/741 (97%)\n",
      ", Recall: 0.9721857492997199, Precision: 0.9745674486803519\n",
      "\n",
      "Train Epoch: 19 [0/5930 (0%)]\tLoss: 0.126488\n",
      "Train Epoch: 19 [2000/5930 (33%)]\tLoss: 0.125735\n",
      "Train Epoch: 19 [4000/5930 (67%)]\tLoss: 0.117587\n",
      "\n",
      "Validate set: Average loss: 0.1271, Accuracy: 721/741 (97%)\n",
      ", Recall: 0.9721857492997199, Precision: 0.9745674486803519\n",
      "\n",
      "Train Epoch: 20 [0/5930 (0%)]\tLoss: 0.115831\n",
      "Train Epoch: 20 [2000/5930 (33%)]\tLoss: 0.125615\n",
      "Train Epoch: 20 [4000/5930 (67%)]\tLoss: 0.115486\n",
      "\n",
      "Validate set: Average loss: 0.1267, Accuracy: 721/741 (97%)\n",
      ", Recall: 0.9721857492997199, Precision: 0.9745674486803519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelCnnBinary = CNNBinary().to(device)\n",
    "\n",
    "args = {'lr': 0.5,\n",
    "              'gamma': 0.7,\n",
    "              'dry-run': False,\n",
    "              'log_interval': 2,\n",
    "              'epochs': 20\n",
    "             }\n",
    "\n",
    "optimizer = torch.optim.Adadelta(modelCnnBinary.parameters(), lr=args['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args['gamma'])\n",
    "\n",
    "test_result = pd.DataFrame(columns=['Epoch','Loss','Accuracy'])\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train(args, modelCnnBinary, device, trainLoader, optimizer, epoch)\n",
    "    (loss, accuracy) = validate(modelCnnBinary, device, validateLoader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1207, Accuracy: 728/742 (98%)\n",
      ", Recall: 0.9801530612244898, Precision: 0.9824316011482805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(loss, accuracy) = test(modelCnnBinary, device, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 VPN/non-VPN binary classification - RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBinary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNBinary, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(1500, 512, 1, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        # Full Connect\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5930 (0%)]\tLoss: 0.690398\n",
      "Train Epoch: 1 [2000/5930 (33%)]\tLoss: 0.525243\n",
      "Train Epoch: 1 [4000/5930 (67%)]\tLoss: 0.709733\n",
      "\n",
      "Validate set: Average loss: 0.4442, Accuracy: 568/741 (77%)\n",
      ", Recall: 0.8317120622568093, Precision: 0.78375\n",
      "\n",
      "Train Epoch: 2 [0/5930 (0%)]\tLoss: 0.441964\n",
      "Train Epoch: 2 [2000/5930 (33%)]\tLoss: 0.326201\n",
      "Train Epoch: 2 [4000/5930 (67%)]\tLoss: 0.254647\n",
      "\n",
      "Validate set: Average loss: 0.4196, Accuracy: 571/741 (77%)\n",
      ", Recall: 0.8336594911937378, Precision: 0.7875\n",
      "\n",
      "Train Epoch: 3 [0/5930 (0%)]\tLoss: 0.414506\n",
      "Train Epoch: 3 [2000/5930 (33%)]\tLoss: 0.283545\n",
      "Train Epoch: 3 [4000/5930 (67%)]\tLoss: 0.194052\n",
      "\n",
      "Validate set: Average loss: 0.1787, Accuracy: 698/741 (94%)\n",
      ", Recall: 0.9440104166666667, Precision: 0.94625\n",
      "\n",
      "Train Epoch: 4 [0/5930 (0%)]\tLoss: 0.166646\n",
      "Train Epoch: 4 [2000/5930 (33%)]\tLoss: 0.122048\n",
      "Train Epoch: 4 [4000/5930 (67%)]\tLoss: 0.101737\n",
      "\n",
      "Validate set: Average loss: 0.1041, Accuracy: 723/741 (98%)\n",
      ", Recall: 0.9749303621169916, Precision: 0.9775\n",
      "\n",
      "Train Epoch: 5 [0/5930 (0%)]\tLoss: 0.085831\n",
      "Train Epoch: 5 [2000/5930 (33%)]\tLoss: 0.087437\n",
      "Train Epoch: 5 [4000/5930 (67%)]\tLoss: 0.077518\n",
      "\n",
      "Validate set: Average loss: 0.0857, Accuracy: 726/741 (98%)\n",
      ", Recall: 0.9789325842696629, Precision: 0.98125\n",
      "\n",
      "Train Epoch: 6 [0/5930 (0%)]\tLoss: 0.068956\n",
      "Train Epoch: 6 [2000/5930 (33%)]\tLoss: 0.072225\n",
      "Train Epoch: 6 [4000/5930 (67%)]\tLoss: 0.063946\n",
      "\n",
      "Validate set: Average loss: 0.0764, Accuracy: 728/741 (98%)\n",
      ", Recall: 0.9816691984108437, Precision: 0.9835337243401759\n",
      "\n",
      "Train Epoch: 7 [0/5930 (0%)]\tLoss: 0.053714\n",
      "Train Epoch: 7 [2000/5930 (33%)]\tLoss: 0.053228\n",
      "Train Epoch: 7 [4000/5930 (67%)]\tLoss: 0.054648\n",
      "\n",
      "Validate set: Average loss: 0.0726, Accuracy: 725/741 (98%)\n",
      ", Recall: 0.977577902649055, Precision: 0.9797837243401759\n",
      "\n",
      "Train Epoch: 8 [0/5930 (0%)]\tLoss: 0.050078\n",
      "Train Epoch: 8 [2000/5930 (33%)]\tLoss: 0.051084\n",
      "Train Epoch: 8 [4000/5930 (67%)]\tLoss: 0.046560\n",
      "\n",
      "Validate set: Average loss: 0.0676, Accuracy: 730/741 (99%)\n",
      ", Recall: 0.984375, Precision: 0.9862500000000001\n",
      "\n",
      "Train Epoch: 9 [0/5930 (0%)]\tLoss: 0.045286\n",
      "Train Epoch: 9 [2000/5930 (33%)]\tLoss: 0.046345\n",
      "Train Epoch: 9 [4000/5930 (67%)]\tLoss: 0.040278\n",
      "\n",
      "Validate set: Average loss: 0.0660, Accuracy: 730/741 (99%)\n",
      ", Recall: 0.984375, Precision: 0.9862500000000001\n",
      "\n",
      "Train Epoch: 10 [0/5930 (0%)]\tLoss: 0.041924\n",
      "Train Epoch: 10 [2000/5930 (33%)]\tLoss: 0.043950\n",
      "Train Epoch: 10 [4000/5930 (67%)]\tLoss: 0.041812\n",
      "\n",
      "Validate set: Average loss: 0.0649, Accuracy: 728/741 (98%)\n",
      ", Recall: 0.981638418079096, Precision: 0.98375\n",
      "\n",
      "Train Epoch: 11 [0/5930 (0%)]\tLoss: 0.045444\n",
      "Train Epoch: 11 [2000/5930 (33%)]\tLoss: 0.034720\n",
      "Train Epoch: 11 [4000/5930 (67%)]\tLoss: 0.045531\n",
      "\n",
      "Validate set: Average loss: 0.0636, Accuracy: 727/741 (98%)\n",
      ", Recall: 0.9802816901408451, Precision: 0.9824999999999999\n",
      "\n",
      "Train Epoch: 12 [0/5930 (0%)]\tLoss: 0.040439\n",
      "Train Epoch: 12 [2000/5930 (33%)]\tLoss: 0.035507\n",
      "Train Epoch: 12 [4000/5930 (67%)]\tLoss: 0.043545\n",
      "\n",
      "Validate set: Average loss: 0.0634, Accuracy: 729/741 (98%)\n",
      ", Recall: 0.9830028328611897, Precision: 0.985\n",
      "\n",
      "Train Epoch: 13 [0/5930 (0%)]\tLoss: 0.041557\n",
      "Train Epoch: 13 [2000/5930 (33%)]\tLoss: 0.038864\n",
      "Train Epoch: 13 [4000/5930 (67%)]\tLoss: 0.034534\n",
      "\n",
      "Validate set: Average loss: 0.0635, Accuracy: 729/741 (98%)\n",
      ", Recall: 0.9830028328611897, Precision: 0.985\n",
      "\n",
      "Train Epoch: 14 [0/5930 (0%)]\tLoss: 0.040826\n",
      "Train Epoch: 14 [2000/5930 (33%)]\tLoss: 0.038810\n",
      "Train Epoch: 14 [4000/5930 (67%)]\tLoss: 0.035424\n",
      "\n",
      "Validate set: Average loss: 0.0630, Accuracy: 726/741 (98%)\n",
      ", Recall: 0.9789339990364823, Precision: 0.9810337243401759\n",
      "\n",
      "Train Epoch: 15 [0/5930 (0%)]\tLoss: 0.039458\n",
      "Train Epoch: 15 [2000/5930 (33%)]\tLoss: 0.036661\n",
      "Train Epoch: 15 [4000/5930 (67%)]\tLoss: 0.036594\n",
      "\n",
      "Validate set: Average loss: 0.0627, Accuracy: 728/741 (98%)\n",
      ", Recall: 0.9816691984108437, Precision: 0.9835337243401759\n",
      "\n",
      "Train Epoch: 16 [0/5930 (0%)]\tLoss: 0.040165\n",
      "Train Epoch: 16 [2000/5930 (33%)]\tLoss: 0.040360\n",
      "Train Epoch: 16 [4000/5930 (67%)]\tLoss: 0.036263\n",
      "\n",
      "Validate set: Average loss: 0.0628, Accuracy: 727/741 (98%)\n",
      ", Recall: 0.9802816901408451, Precision: 0.9824999999999999\n",
      "\n",
      "Train Epoch: 17 [0/5930 (0%)]\tLoss: 0.034084\n",
      "Train Epoch: 17 [2000/5930 (33%)]\tLoss: 0.039650\n",
      "Train Epoch: 17 [4000/5930 (67%)]\tLoss: 0.035049\n",
      "\n",
      "Validate set: Average loss: 0.0624, Accuracy: 726/741 (98%)\n",
      ", Recall: 0.9789339990364823, Precision: 0.9810337243401759\n",
      "\n",
      "Train Epoch: 18 [0/5930 (0%)]\tLoss: 0.031445\n",
      "Train Epoch: 18 [2000/5930 (33%)]\tLoss: 0.040923\n",
      "Train Epoch: 18 [4000/5930 (67%)]\tLoss: 0.036014\n",
      "\n",
      "Validate set: Average loss: 0.0611, Accuracy: 729/741 (98%)\n",
      ", Recall: 0.9830028328611897, Precision: 0.985\n",
      "\n",
      "Train Epoch: 19 [0/5930 (0%)]\tLoss: 0.040683\n",
      "Train Epoch: 19 [2000/5930 (33%)]\tLoss: 0.037800\n",
      "Train Epoch: 19 [4000/5930 (67%)]\tLoss: 0.037315\n",
      "\n",
      "Validate set: Average loss: 0.0609, Accuracy: 729/741 (98%)\n",
      ", Recall: 0.9830028328611897, Precision: 0.985\n",
      "\n",
      "Train Epoch: 20 [0/5930 (0%)]\tLoss: 0.033699\n",
      "Train Epoch: 20 [2000/5930 (33%)]\tLoss: 0.040274\n",
      "Train Epoch: 20 [4000/5930 (67%)]\tLoss: 0.039617\n",
      "\n",
      "Validate set: Average loss: 0.0612, Accuracy: 728/741 (98%)\n",
      ", Recall: 0.981638418079096, Precision: 0.98375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelRnnBinary = RNNBinary().to(device)\n",
    "\n",
    "args = {'lr': 0.5,\n",
    "              'gamma': 0.7,\n",
    "              'dry-run': False,\n",
    "              'log_interval': 2,\n",
    "              'epochs': 20\n",
    "             }\n",
    "\n",
    "optimizer = torch.optim.Adadelta(modelRnnBinary.parameters(), lr=args['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args['gamma'])\n",
    "\n",
    "test_result = pd.DataFrame(columns=['Epoch','Loss','Accuracy'])\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train(args, modelRnnBinary, device, trainLoader, optimizer, epoch)\n",
    "    (loss, accuracy) = validate(modelRnnBinary, device, validateLoader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0634, Accuracy: 729/742 (98%)\n",
      ", Recall: 0.9814814814814814, Precision: 0.9839108910891089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(loss, accuracy) = test(modelRnnBinary, device, testLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Detailed traffic multi classification - dataloader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpnDf = pd.concat(vpnDataframes, ignore_index=True, sort=False)\n",
    "nonVpnDf = pd.concat(nonVpnDataframes, ignore_index=True, sort=False)\n",
    "\n",
    "vpnDfLabels = list(set(vpnDf['Label'].to_list()))\n",
    "vpnDf['Label'] = vpnDf['Label'].apply(lambda x: vpnDfLabels.index(x))\n",
    "vpnTrainDf, vpnValidateDf, vpnTestDf = np.split(vpnDf.sample(frac=1), [int(.8*len(vpnDf)), int(.9*len(vpnDf))])\n",
    "\n",
    "nonVpnDfLabels = list(set(nonVpnDf['Label'].to_list()))\n",
    "nonVpnDf['Label'] = nonVpnDf['Label'].apply(lambda x: nonVpnDfLabels.index(x))\n",
    "nonVpnTrainDf, nonVpnValidateDf, nonVpnTestDf = np.split(nonVpnDf.sample(frac=1), [int(.8*len(nonVpnDf)), int(.9*len(nonVpnDf))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3187, 1501),\n",
       " (398, 1501),\n",
       " (399, 1501),\n",
       " ['VPN-Email', 'VPN-P2P', 'VPN-Chat', 'VPN-Streaming'],\n",
       " ['P2P', 'Chat', 'Streaming', 'Email'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpnTrainDf.shape, vpnValidateDf.shape, vpnTestDf.shape, vpnDfLabels, nonVpnDfLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataIndex_0</th>\n",
       "      <th>DataIndex_1</th>\n",
       "      <th>DataIndex_2</th>\n",
       "      <th>DataIndex_3</th>\n",
       "      <th>DataIndex_4</th>\n",
       "      <th>DataIndex_5</th>\n",
       "      <th>DataIndex_6</th>\n",
       "      <th>DataIndex_7</th>\n",
       "      <th>DataIndex_8</th>\n",
       "      <th>DataIndex_9</th>\n",
       "      <th>...</th>\n",
       "      <th>DataIndex_1491</th>\n",
       "      <th>DataIndex_1492</th>\n",
       "      <th>DataIndex_1493</th>\n",
       "      <th>DataIndex_1494</th>\n",
       "      <th>DataIndex_1495</th>\n",
       "      <th>DataIndex_1496</th>\n",
       "      <th>DataIndex_1497</th>\n",
       "      <th>DataIndex_1498</th>\n",
       "      <th>DataIndex_1499</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>239</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>103</td>\n",
       "      <td>108</td>\n",
       "      <td>101</td>\n",
       "      <td>46</td>\n",
       "      <td>105</td>\n",
       "      <td>116</td>\n",
       "      <td>130</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>193</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>244</td>\n",
       "      <td>201</td>\n",
       "      <td>171</td>\n",
       "      <td>154</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>126</td>\n",
       "      <td>242</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>163</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>239</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>239</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3819</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>199</td>\n",
       "      <td>187</td>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>217</td>\n",
       "      <td>227</td>\n",
       "      <td>41</td>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>161</td>\n",
       "      <td>178</td>\n",
       "      <td>195</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>224</td>\n",
       "      <td>255</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>130</td>\n",
       "      <td>173</td>\n",
       "      <td>194</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3187 rows × 1501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DataIndex_0  DataIndex_1  DataIndex_2  DataIndex_3  DataIndex_4  \\\n",
       "1886          161          178          195          212            0   \n",
       "864           161          178          195          212            0   \n",
       "3869          161          178          195          212            0   \n",
       "3150          161          178          195          212            0   \n",
       "256           161          178          195          212            0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "343           161          178          195          212            0   \n",
       "1691          161          178          195          212            0   \n",
       "2005          161          178          195          212            0   \n",
       "3819          161          178          195          212            0   \n",
       "1503          161          178          195          212            0   \n",
       "\n",
       "      DataIndex_5  DataIndex_6  DataIndex_7  DataIndex_8  DataIndex_9  ...  \\\n",
       "1886            2            0            4            0            0  ...   \n",
       "864             2            0            4            0            0  ...   \n",
       "3869            2            0            4            0            0  ...   \n",
       "3150            2            0            4            0            0  ...   \n",
       "256             2            0            4            0            0  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "343             2            0            4            0            0  ...   \n",
       "1691            2            0            4            0            0  ...   \n",
       "2005            2            0            4            0            0  ...   \n",
       "3819            2            0            4            0            0  ...   \n",
       "1503            2            0            4            0            0  ...   \n",
       "\n",
       "      DataIndex_1491  DataIndex_1492  DataIndex_1493  DataIndex_1494  \\\n",
       "1886               0               0               0               0   \n",
       "864                4             216             239              38   \n",
       "3869             111             103             108             101   \n",
       "3150             193               8              16              54   \n",
       "256                0               0               0               0   \n",
       "...              ...             ...             ...             ...   \n",
       "343               65             126             242              64   \n",
       "1691               4             216             239              38   \n",
       "2005               4             216             239              38   \n",
       "3819             199             187             104               5   \n",
       "1503               6             224             255              10   \n",
       "\n",
       "      DataIndex_1495  DataIndex_1496  DataIndex_1497  DataIndex_1498  \\\n",
       "1886               0               1              48              14   \n",
       "864               10             161             178             195   \n",
       "3869              46             105             116             130   \n",
       "3150             244             201             171             154   \n",
       "256                0               1              48              14   \n",
       "...              ...             ...             ...             ...   \n",
       "343                0              64              17             163   \n",
       "1691              10             161             178             195   \n",
       "2005              10             161             178             195   \n",
       "3819             217             227              41              70   \n",
       "1503               8               8             130             173   \n",
       "\n",
       "      DataIndex_1499  Label  \n",
       "1886              99      2  \n",
       "864              212      2  \n",
       "3869              11      3  \n",
       "3150             249      0  \n",
       "256               99      2  \n",
       "...              ...    ...  \n",
       "343              107      2  \n",
       "1691             212      2  \n",
       "2005             212      2  \n",
       "3819              77      3  \n",
       "1503             194      2  \n",
       "\n",
       "[3187 rows x 1501 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpnTrainDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDataset():\n",
    "\n",
    "    def __init__(self, dataframe, transform=None, target_transform = None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = np.array([self.df.iloc[index, :-1]])\n",
    "        data = torch.from_numpy(data).view(1, df.shape[1] - 1).float()\n",
    "        target = self.df.iloc[index, -1]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vpnTrainDataset = MultiDataset(vpnTrainDf)\n",
    "vpnValidateDataset = MultiDataset(vpnValidateDf)\n",
    "vpnTestDataset = MultiDataset(vpnTestDf)\n",
    "\n",
    "loaderArgs = {'batch_size': 200}\n",
    "\n",
    "if use_cuda:\n",
    "    loaderArgs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "                     )\n",
    "else:\n",
    "    loaderArgs.update({'shuffle': True})\n",
    "    \n",
    "vpnTrainLoader = torch.utils.data.DataLoader(vpnTrainDataset,**loaderArgs)\n",
    "vpnValidateLoader = torch.utils.data.DataLoader(vpnValidateDataset, **loaderArgs)\n",
    "vpnTestLoader = torch.utils.data.DataLoader(vpnTestDataset, **loaderArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonVpnTrainDataset = MultiDataset(nonVpnTrainDf)\n",
    "nonVpnValidateDataset = MultiDataset(nonVpnValidateDf)\n",
    "nonVpnTestDataset = MultiDataset(nonVpnTestDf)\n",
    "\n",
    "loaderArgs = {'batch_size': 200}\n",
    "\n",
    "if use_cuda:\n",
    "    loaderArgs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "                     )\n",
    "else:\n",
    "    loaderArgs.update({'shuffle': True})\n",
    "    \n",
    "nonVpnTrainLoader = torch.utils.data.DataLoader(nonVpnTrainDataset,**loaderArgs)\n",
    "nonVpnValidateLoader = torch.utils.data.DataLoader(nonVpnValidateDataset, **loaderArgs)\n",
    "nonVpnTestLoader = torch.utils.data.DataLoader(nonVpnTestDataset, **loaderArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Detailed traffic multi classification - RNN model - VPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNMulti(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNMulti, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(1500, 512, 1, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        # Full Connect\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3187 (0%)]\tLoss: 1.408167\n",
      "Train Epoch: 1 [400/3187 (12%)]\tLoss: 0.829357\n",
      "Train Epoch: 1 [800/3187 (25%)]\tLoss: 0.746715\n",
      "Train Epoch: 1 [1200/3187 (38%)]\tLoss: 0.628626\n",
      "Train Epoch: 1 [1600/3187 (50%)]\tLoss: 0.504486\n",
      "Train Epoch: 1 [2000/3187 (62%)]\tLoss: 0.568432\n",
      "Train Epoch: 1 [2400/3187 (75%)]\tLoss: 0.536760\n",
      "Train Epoch: 1 [2800/3187 (88%)]\tLoss: 0.457348\n",
      "\n",
      "Test set: Average loss: 0.4453, Accuracy: 342/398 (86%)\n",
      ", Recall: 0.8061075762763448, Precision: 0.5953007335643992\n",
      "\n",
      "Train Epoch: 2 [0/3187 (0%)]\tLoss: 0.467788\n",
      "Train Epoch: 2 [400/3187 (12%)]\tLoss: 0.464757\n",
      "Train Epoch: 2 [800/3187 (25%)]\tLoss: 0.343220\n",
      "Train Epoch: 2 [1200/3187 (38%)]\tLoss: 0.337430\n",
      "Train Epoch: 2 [1600/3187 (50%)]\tLoss: 0.340210\n",
      "Train Epoch: 2 [2000/3187 (62%)]\tLoss: 0.397777\n",
      "Train Epoch: 2 [2400/3187 (75%)]\tLoss: 0.331102\n",
      "Train Epoch: 2 [2800/3187 (88%)]\tLoss: 0.414150\n",
      "\n",
      "Test set: Average loss: 0.3337, Accuracy: 346/398 (87%)\n",
      ", Recall: 0.9641873278236914, Precision: 0.5887445887445888\n",
      "\n",
      "Train Epoch: 3 [0/3187 (0%)]\tLoss: 0.378599\n",
      "Train Epoch: 3 [400/3187 (12%)]\tLoss: 0.278318\n",
      "Train Epoch: 3 [800/3187 (25%)]\tLoss: 0.215143\n",
      "Train Epoch: 3 [1200/3187 (38%)]\tLoss: 0.266352\n",
      "Train Epoch: 3 [1600/3187 (50%)]\tLoss: 0.272912\n",
      "Train Epoch: 3 [2000/3187 (62%)]\tLoss: 0.339002\n",
      "Train Epoch: 3 [2400/3187 (75%)]\tLoss: 0.286364\n",
      "Train Epoch: 3 [2800/3187 (88%)]\tLoss: 0.281783\n",
      "\n",
      "Test set: Average loss: 0.2274, Accuracy: 372/398 (93%)\n",
      ", Recall: 0.9228209688783489, Precision: 0.8060299828788574\n",
      "\n",
      "Train Epoch: 4 [0/3187 (0%)]\tLoss: 0.231390\n",
      "Train Epoch: 4 [400/3187 (12%)]\tLoss: 0.186646\n",
      "Train Epoch: 4 [800/3187 (25%)]\tLoss: 0.246653\n",
      "Train Epoch: 4 [1200/3187 (38%)]\tLoss: 0.198274\n",
      "Train Epoch: 4 [1600/3187 (50%)]\tLoss: 0.297506\n",
      "Train Epoch: 4 [2000/3187 (62%)]\tLoss: 0.207609\n",
      "Train Epoch: 4 [2400/3187 (75%)]\tLoss: 0.247260\n",
      "Train Epoch: 4 [2800/3187 (88%)]\tLoss: 0.175599\n",
      "\n",
      "Test set: Average loss: 0.1912, Accuracy: 376/398 (94%)\n",
      ", Recall: 0.9363722697056032, Precision: 0.8406620175108921\n",
      "\n",
      "Train Epoch: 5 [0/3187 (0%)]\tLoss: 0.262679\n",
      "Train Epoch: 5 [400/3187 (12%)]\tLoss: 0.142498\n",
      "Train Epoch: 5 [800/3187 (25%)]\tLoss: 0.192080\n",
      "Train Epoch: 5 [1200/3187 (38%)]\tLoss: 0.200469\n",
      "Train Epoch: 5 [1600/3187 (50%)]\tLoss: 0.172904\n",
      "Train Epoch: 5 [2000/3187 (62%)]\tLoss: 0.194938\n",
      "Train Epoch: 5 [2400/3187 (75%)]\tLoss: 0.167805\n",
      "Train Epoch: 5 [2800/3187 (88%)]\tLoss: 0.161689\n",
      "\n",
      "Test set: Average loss: 0.1703, Accuracy: 376/398 (94%)\n",
      ", Recall: 0.9336085311317509, Precision: 0.8295611141270305\n",
      "\n",
      "Train Epoch: 6 [0/3187 (0%)]\tLoss: 0.186728\n",
      "Train Epoch: 6 [400/3187 (12%)]\tLoss: 0.187294\n",
      "Train Epoch: 6 [800/3187 (25%)]\tLoss: 0.170948\n",
      "Train Epoch: 6 [1200/3187 (38%)]\tLoss: 0.217400\n",
      "Train Epoch: 6 [1600/3187 (50%)]\tLoss: 0.122475\n",
      "Train Epoch: 6 [2000/3187 (62%)]\tLoss: 0.122551\n",
      "Train Epoch: 6 [2400/3187 (75%)]\tLoss: 0.092365\n",
      "Train Epoch: 6 [2800/3187 (88%)]\tLoss: 0.152592\n",
      "\n",
      "Test set: Average loss: 0.1599, Accuracy: 378/398 (95%)\n",
      ", Recall: 0.9493634259259259, Precision: 0.8490416336075499\n",
      "\n",
      "Train Epoch: 7 [0/3187 (0%)]\tLoss: 0.133090\n",
      "Train Epoch: 7 [400/3187 (12%)]\tLoss: 0.141741\n",
      "Train Epoch: 7 [800/3187 (25%)]\tLoss: 0.124826\n",
      "Train Epoch: 7 [1200/3187 (38%)]\tLoss: 0.131928\n",
      "Train Epoch: 7 [1600/3187 (50%)]\tLoss: 0.122350\n",
      "Train Epoch: 7 [2000/3187 (62%)]\tLoss: 0.098597\n",
      "Train Epoch: 7 [2400/3187 (75%)]\tLoss: 0.131450\n",
      "Train Epoch: 7 [2800/3187 (88%)]\tLoss: 0.129227\n",
      "\n",
      "Test set: Average loss: 0.1515, Accuracy: 380/398 (95%)\n",
      ", Recall: 0.9365547489413188, Precision: 0.8709650478139224\n",
      "\n",
      "Train Epoch: 8 [0/3187 (0%)]\tLoss: 0.105108\n",
      "Train Epoch: 8 [400/3187 (12%)]\tLoss: 0.088463\n",
      "Train Epoch: 8 [800/3187 (25%)]\tLoss: 0.096491\n",
      "Train Epoch: 8 [1200/3187 (38%)]\tLoss: 0.106196\n",
      "Train Epoch: 8 [1600/3187 (50%)]\tLoss: 0.119174\n",
      "Train Epoch: 8 [2000/3187 (62%)]\tLoss: 0.139242\n",
      "Train Epoch: 8 [2400/3187 (75%)]\tLoss: 0.091539\n",
      "Train Epoch: 8 [2800/3187 (88%)]\tLoss: 0.103682\n",
      "\n",
      "Test set: Average loss: 0.1503, Accuracy: 380/398 (95%)\n",
      ", Recall: 0.9365547489413188, Precision: 0.8709650478139224\n",
      "\n",
      "Train Epoch: 9 [0/3187 (0%)]\tLoss: 0.129331\n",
      "Train Epoch: 9 [400/3187 (12%)]\tLoss: 0.094482\n",
      "Train Epoch: 9 [800/3187 (25%)]\tLoss: 0.110962\n",
      "Train Epoch: 9 [1200/3187 (38%)]\tLoss: 0.139118\n",
      "Train Epoch: 9 [1600/3187 (50%)]\tLoss: 0.081653\n",
      "Train Epoch: 9 [2000/3187 (62%)]\tLoss: 0.061665\n",
      "Train Epoch: 9 [2400/3187 (75%)]\tLoss: 0.077053\n",
      "Train Epoch: 9 [2800/3187 (88%)]\tLoss: 0.107901\n",
      "\n",
      "Test set: Average loss: 0.1484, Accuracy: 378/398 (95%)\n",
      ", Recall: 0.9328465732087228, Precision: 0.8625854317172645\n",
      "\n",
      "Train Epoch: 10 [0/3187 (0%)]\tLoss: 0.079184\n",
      "Train Epoch: 10 [400/3187 (12%)]\tLoss: 0.074343\n",
      "Train Epoch: 10 [800/3187 (25%)]\tLoss: 0.101754\n",
      "Train Epoch: 10 [1200/3187 (38%)]\tLoss: 0.147379\n",
      "Train Epoch: 10 [1600/3187 (50%)]\tLoss: 0.098939\n",
      "Train Epoch: 10 [2000/3187 (62%)]\tLoss: 0.150760\n",
      "Train Epoch: 10 [2400/3187 (75%)]\tLoss: 0.083149\n",
      "Train Epoch: 10 [2800/3187 (88%)]\tLoss: 0.136558\n",
      "\n",
      "Test set: Average loss: 0.1409, Accuracy: 381/398 (96%)\n",
      ", Recall: 0.937289003265261, Precision: 0.8853127044445372\n",
      "\n",
      "Train Epoch: 11 [0/3187 (0%)]\tLoss: 0.096158\n",
      "Train Epoch: 11 [400/3187 (12%)]\tLoss: 0.068975\n",
      "Train Epoch: 11 [800/3187 (25%)]\tLoss: 0.083182\n",
      "Train Epoch: 11 [1200/3187 (38%)]\tLoss: 0.107254\n",
      "Train Epoch: 11 [1600/3187 (50%)]\tLoss: 0.104045\n",
      "Train Epoch: 11 [2000/3187 (62%)]\tLoss: 0.073760\n",
      "Train Epoch: 11 [2400/3187 (75%)]\tLoss: 0.070765\n",
      "Train Epoch: 11 [2800/3187 (88%)]\tLoss: 0.104029\n",
      "\n",
      "Test set: Average loss: 0.1431, Accuracy: 379/398 (95%)\n",
      ", Recall: 0.9456695097427363, Precision: 0.8633892902381648\n",
      "\n",
      "Train Epoch: 12 [0/3187 (0%)]\tLoss: 0.079983\n",
      "Train Epoch: 12 [400/3187 (12%)]\tLoss: 0.095812\n",
      "Train Epoch: 12 [800/3187 (25%)]\tLoss: 0.115772\n",
      "Train Epoch: 12 [1200/3187 (38%)]\tLoss: 0.096572\n",
      "Train Epoch: 12 [1600/3187 (50%)]\tLoss: 0.074452\n",
      "Train Epoch: 12 [2000/3187 (62%)]\tLoss: 0.089247\n",
      "Train Epoch: 12 [2400/3187 (75%)]\tLoss: 0.063746\n",
      "Train Epoch: 12 [2800/3187 (88%)]\tLoss: 0.085005\n",
      "\n",
      "Test set: Average loss: 0.1421, Accuracy: 382/398 (96%)\n",
      ", Recall: 0.9432661843287464, Precision: 0.8861165629654375\n",
      "\n",
      "Train Epoch: 13 [0/3187 (0%)]\tLoss: 0.104490\n",
      "Train Epoch: 13 [400/3187 (12%)]\tLoss: 0.073334\n",
      "Train Epoch: 13 [800/3187 (25%)]\tLoss: 0.045445\n",
      "Train Epoch: 13 [1200/3187 (38%)]\tLoss: 0.118226\n",
      "Train Epoch: 13 [1600/3187 (50%)]\tLoss: 0.065603\n",
      "Train Epoch: 13 [2000/3187 (62%)]\tLoss: 0.075467\n",
      "Train Epoch: 13 [2400/3187 (75%)]\tLoss: 0.100729\n",
      "Train Epoch: 13 [2800/3187 (88%)]\tLoss: 0.076082\n",
      "\n",
      "Test set: Average loss: 0.1416, Accuracy: 380/398 (95%)\n",
      ", Recall: 0.9417234308341387, Precision: 0.8777369468687797\n",
      "\n",
      "Train Epoch: 14 [0/3187 (0%)]\tLoss: 0.103241\n",
      "Train Epoch: 14 [400/3187 (12%)]\tLoss: 0.104004\n",
      "Train Epoch: 14 [800/3187 (25%)]\tLoss: 0.065834\n",
      "Train Epoch: 14 [1200/3187 (38%)]\tLoss: 0.067703\n",
      "Train Epoch: 14 [1600/3187 (50%)]\tLoss: 0.093479\n",
      "Train Epoch: 14 [2000/3187 (62%)]\tLoss: 0.101259\n",
      "Train Epoch: 14 [2400/3187 (75%)]\tLoss: 0.101853\n",
      "Train Epoch: 14 [2800/3187 (88%)]\tLoss: 0.072921\n",
      "\n",
      "Test set: Average loss: 0.1402, Accuracy: 377/398 (95%)\n",
      ", Recall: 0.9280429063777521, Precision: 0.8617815731963643\n",
      "\n",
      "Train Epoch: 15 [0/3187 (0%)]\tLoss: 0.109997\n",
      "Train Epoch: 15 [400/3187 (12%)]\tLoss: 0.116905\n",
      "Train Epoch: 15 [800/3187 (25%)]\tLoss: 0.046175\n",
      "Train Epoch: 15 [1200/3187 (38%)]\tLoss: 0.092185\n",
      "Train Epoch: 15 [1600/3187 (50%)]\tLoss: 0.057783\n",
      "Train Epoch: 15 [2000/3187 (62%)]\tLoss: 0.091231\n",
      "Train Epoch: 15 [2400/3187 (75%)]\tLoss: 0.081640\n",
      "Train Epoch: 15 [2800/3187 (88%)]\tLoss: 0.058458\n",
      "\n",
      "Test set: Average loss: 0.1426, Accuracy: 380/398 (95%)\n",
      ", Recall: 0.9464144643682273, Precision: 0.8709650478139224\n",
      "\n",
      "Train Epoch: 16 [0/3187 (0%)]\tLoss: 0.068624\n",
      "Train Epoch: 16 [400/3187 (12%)]\tLoss: 0.078108\n",
      "Train Epoch: 16 [800/3187 (25%)]\tLoss: 0.061212\n",
      "Train Epoch: 16 [1200/3187 (38%)]\tLoss: 0.066046\n",
      "Train Epoch: 16 [1600/3187 (50%)]\tLoss: 0.086509\n",
      "Train Epoch: 16 [2000/3187 (62%)]\tLoss: 0.054313\n",
      "Train Epoch: 16 [2400/3187 (75%)]\tLoss: 0.099936\n",
      "Train Epoch: 16 [2800/3187 (88%)]\tLoss: 0.073941\n",
      "\n",
      "Test set: Average loss: 0.1427, Accuracy: 378/398 (95%)\n",
      ", Recall: 0.9180870461392103, Precision: 0.8693573307721219\n",
      "\n",
      "Train Epoch: 17 [0/3187 (0%)]\tLoss: 0.124288\n",
      "Train Epoch: 17 [400/3187 (12%)]\tLoss: 0.054913\n",
      "Train Epoch: 17 [800/3187 (25%)]\tLoss: 0.084821\n",
      "Train Epoch: 17 [1200/3187 (38%)]\tLoss: 0.065755\n",
      "Train Epoch: 17 [1600/3187 (50%)]\tLoss: 0.144668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [2000/3187 (62%)]\tLoss: 0.101656\n",
      "Train Epoch: 17 [2400/3187 (75%)]\tLoss: 0.116610\n",
      "Train Epoch: 17 [2800/3187 (88%)]\tLoss: 0.051962\n",
      "\n",
      "Test set: Average loss: 0.1412, Accuracy: 379/398 (95%)\n",
      ", Recall: 0.9288309054029136, Precision: 0.8701611892930221\n",
      "\n",
      "Train Epoch: 18 [0/3187 (0%)]\tLoss: 0.074374\n",
      "Train Epoch: 18 [400/3187 (12%)]\tLoss: 0.063801\n",
      "Train Epoch: 18 [800/3187 (25%)]\tLoss: 0.072350\n",
      "Train Epoch: 18 [1200/3187 (38%)]\tLoss: 0.086313\n",
      "Train Epoch: 18 [1600/3187 (50%)]\tLoss: 0.074265\n",
      "Train Epoch: 18 [2000/3187 (62%)]\tLoss: 0.100659\n",
      "Train Epoch: 18 [2400/3187 (75%)]\tLoss: 0.059017\n",
      "Train Epoch: 18 [2800/3187 (88%)]\tLoss: 0.131128\n",
      "\n",
      "Test set: Average loss: 0.1383, Accuracy: 380/398 (95%)\n",
      ", Recall: 0.93075069144377, Precision: 0.8888378502526413\n",
      "\n",
      "Train Epoch: 19 [0/3187 (0%)]\tLoss: 0.058016\n",
      "Train Epoch: 19 [400/3187 (12%)]\tLoss: 0.101049\n",
      "Train Epoch: 19 [800/3187 (25%)]\tLoss: 0.064488\n",
      "Train Epoch: 19 [1200/3187 (38%)]\tLoss: 0.081268\n",
      "Train Epoch: 19 [1600/3187 (50%)]\tLoss: 0.096709\n",
      "Train Epoch: 19 [2000/3187 (62%)]\tLoss: 0.086989\n",
      "Train Epoch: 19 [2400/3187 (75%)]\tLoss: 0.070549\n",
      "Train Epoch: 19 [2800/3187 (88%)]\tLoss: 0.106222\n",
      "\n",
      "Test set: Average loss: 0.1414, Accuracy: 381/398 (96%)\n",
      ", Recall: 0.9421146044624746, Precision: 0.8828698097186842\n",
      "\n",
      "Train Epoch: 20 [0/3187 (0%)]\tLoss: 0.079516\n",
      "Train Epoch: 20 [400/3187 (12%)]\tLoss: 0.147508\n",
      "Train Epoch: 20 [800/3187 (25%)]\tLoss: 0.068784\n",
      "Train Epoch: 20 [1200/3187 (38%)]\tLoss: 0.074124\n",
      "Train Epoch: 20 [1600/3187 (50%)]\tLoss: 0.070509\n",
      "Train Epoch: 20 [2000/3187 (62%)]\tLoss: 0.073190\n",
      "Train Epoch: 20 [2400/3187 (75%)]\tLoss: 0.058366\n",
      "Train Epoch: 20 [2800/3187 (88%)]\tLoss: 0.124885\n",
      "\n",
      "Test set: Average loss: 0.1412, Accuracy: 379/398 (95%)\n",
      ", Recall: 0.9288013318534961, Precision: 0.8769330883478794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelVpnRnnBinary = RNNMulti().to(device)\n",
    "\n",
    "args = {'lr': 0.5,\n",
    "              'gamma': 0.7,\n",
    "              'dry-run': False,\n",
    "              'log_interval': 2,\n",
    "              'epochs': 20\n",
    "             }\n",
    "\n",
    "optimizer = torch.optim.Adadelta(modelVpnRnnBinary.parameters(), lr=args['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args['gamma'])\n",
    "\n",
    "test_result = pd.DataFrame(columns=['Epoch','Loss','Accuracy'])\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train(args, modelVpnRnnBinary, device, vpnTrainLoader, optimizer, epoch)\n",
    "    (loss, accuracy) = test(modelVpnRnnBinary, device, vpnValidateLoader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1393, Accuracy: 381/399 (95%)\n",
      ", Recall: 0.9201564334200991, Precision: 0.8910653974508463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(loss, accuracy) = test(modelVpnRnnBinary, device, vpnTestLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Detailed traffic multi classification - RNN model - non-VPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2743 (0%)]\tLoss: 1.391385\n",
      "Train Epoch: 1 [400/2743 (14%)]\tLoss: 1.730244\n",
      "Train Epoch: 1 [800/2743 (29%)]\tLoss: 1.402281\n",
      "Train Epoch: 1 [1200/2743 (43%)]\tLoss: 1.325830\n",
      "Train Epoch: 1 [1600/2743 (57%)]\tLoss: 1.514870\n",
      "Train Epoch: 1 [2000/2743 (71%)]\tLoss: 1.283655\n",
      "Train Epoch: 1 [2400/2743 (86%)]\tLoss: 1.262542\n",
      "\n",
      "Test set: Average loss: 1.2831, Accuracy: 124/343 (36%)\n",
      ", Recall: 0.24102564102564103, Precision: 0.36650485436893204\n",
      "\n",
      "Train Epoch: 2 [0/2743 (0%)]\tLoss: 1.287698\n",
      "Train Epoch: 2 [400/2743 (14%)]\tLoss: 1.270572\n",
      "Train Epoch: 2 [800/2743 (29%)]\tLoss: 1.095210\n",
      "Train Epoch: 2 [1200/2743 (43%)]\tLoss: 1.378744\n",
      "Train Epoch: 2 [1600/2743 (57%)]\tLoss: 1.488130\n",
      "Train Epoch: 2 [2000/2743 (71%)]\tLoss: 1.077193\n",
      "Train Epoch: 2 [2400/2743 (86%)]\tLoss: 1.036214\n",
      "\n",
      "Test set: Average loss: 1.0840, Accuracy: 156/343 (45%)\n",
      ", Recall: 0.44008526850507984, Precision: 0.406576774360082\n",
      "\n",
      "Train Epoch: 3 [0/2743 (0%)]\tLoss: 1.085998\n",
      "Train Epoch: 3 [400/2743 (14%)]\tLoss: 0.930400\n",
      "Train Epoch: 3 [800/2743 (29%)]\tLoss: 1.033878\n",
      "Train Epoch: 3 [1200/2743 (43%)]\tLoss: 0.953419\n",
      "Train Epoch: 3 [1600/2743 (57%)]\tLoss: 0.905438\n",
      "Train Epoch: 3 [2000/2743 (71%)]\tLoss: 0.889273\n",
      "Train Epoch: 3 [2400/2743 (86%)]\tLoss: 0.939088\n",
      "\n",
      "Test set: Average loss: 0.9324, Accuracy: 205/343 (60%)\n",
      ", Recall: 0.7310259774975092, Precision: 0.6397339842700536\n",
      "\n",
      "Train Epoch: 4 [0/2743 (0%)]\tLoss: 0.886525\n",
      "Train Epoch: 4 [400/2743 (14%)]\tLoss: 0.743264\n",
      "Train Epoch: 4 [800/2743 (29%)]\tLoss: 0.916100\n",
      "Train Epoch: 4 [1200/2743 (43%)]\tLoss: 0.727041\n",
      "Train Epoch: 4 [1600/2743 (57%)]\tLoss: 0.617361\n",
      "Train Epoch: 4 [2000/2743 (71%)]\tLoss: 0.613955\n",
      "Train Epoch: 4 [2400/2743 (86%)]\tLoss: 0.919600\n",
      "\n",
      "Test set: Average loss: 0.7707, Accuracy: 202/343 (59%)\n",
      ", Recall: 0.7342936850275477, Precision: 0.557347409998142\n",
      "\n",
      "Train Epoch: 5 [0/2743 (0%)]\tLoss: 0.730661\n",
      "Train Epoch: 5 [400/2743 (14%)]\tLoss: 0.562142\n",
      "Train Epoch: 5 [800/2743 (29%)]\tLoss: 0.525833\n",
      "Train Epoch: 5 [1200/2743 (43%)]\tLoss: 0.512261\n",
      "Train Epoch: 5 [1600/2743 (57%)]\tLoss: 0.513158\n",
      "Train Epoch: 5 [2000/2743 (71%)]\tLoss: 0.527754\n",
      "Train Epoch: 5 [2400/2743 (86%)]\tLoss: 0.496030\n",
      "\n",
      "Test set: Average loss: 0.5146, Accuracy: 256/343 (75%)\n",
      ", Recall: 0.7674564581744217, Precision: 0.7703202127790896\n",
      "\n",
      "Train Epoch: 6 [0/2743 (0%)]\tLoss: 0.432304\n",
      "Train Epoch: 6 [400/2743 (14%)]\tLoss: 0.416992\n",
      "Train Epoch: 6 [800/2743 (29%)]\tLoss: 0.389507\n",
      "Train Epoch: 6 [1200/2743 (43%)]\tLoss: 0.454958\n",
      "Train Epoch: 6 [1600/2743 (57%)]\tLoss: 0.421212\n",
      "Train Epoch: 6 [2000/2743 (71%)]\tLoss: 0.421738\n",
      "Train Epoch: 6 [2400/2743 (86%)]\tLoss: 0.389744\n",
      "\n",
      "Test set: Average loss: 0.4624, Accuracy: 276/343 (80%)\n",
      ", Recall: 0.8526563505896023, Precision: 0.8243458095003491\n",
      "\n",
      "Train Epoch: 7 [0/2743 (0%)]\tLoss: 0.355892\n",
      "Train Epoch: 7 [400/2743 (14%)]\tLoss: 0.343168\n",
      "Train Epoch: 7 [800/2743 (29%)]\tLoss: 0.342035\n",
      "Train Epoch: 7 [1200/2743 (43%)]\tLoss: 0.332421\n",
      "Train Epoch: 7 [1600/2743 (57%)]\tLoss: 0.353645\n",
      "Train Epoch: 7 [2000/2743 (71%)]\tLoss: 0.376683\n",
      "Train Epoch: 7 [2400/2743 (86%)]\tLoss: 0.321379\n",
      "\n",
      "Test set: Average loss: 0.4228, Accuracy: 272/343 (79%)\n",
      ", Recall: 0.8118930063677798, Precision: 0.8128229634872485\n",
      "\n",
      "Train Epoch: 8 [0/2743 (0%)]\tLoss: 0.321178\n",
      "Train Epoch: 8 [400/2743 (14%)]\tLoss: 0.310171\n",
      "Train Epoch: 8 [800/2743 (29%)]\tLoss: 0.329594\n",
      "Train Epoch: 8 [1200/2743 (43%)]\tLoss: 0.313029\n",
      "Train Epoch: 8 [1600/2743 (57%)]\tLoss: 0.298701\n",
      "Train Epoch: 8 [2000/2743 (71%)]\tLoss: 0.252513\n",
      "Train Epoch: 8 [2400/2743 (86%)]\tLoss: 0.313420\n",
      "\n",
      "Test set: Average loss: 0.3996, Accuracy: 281/343 (82%)\n",
      ", Recall: 0.8483388765705838, Precision: 0.8367134948161908\n",
      "\n",
      "Train Epoch: 9 [0/2743 (0%)]\tLoss: 0.237455\n",
      "Train Epoch: 9 [400/2743 (14%)]\tLoss: 0.280009\n",
      "Train Epoch: 9 [800/2743 (29%)]\tLoss: 0.256153\n",
      "Train Epoch: 9 [1200/2743 (43%)]\tLoss: 0.256663\n",
      "Train Epoch: 9 [1600/2743 (57%)]\tLoss: 0.236019\n",
      "Train Epoch: 9 [2000/2743 (71%)]\tLoss: 0.270121\n",
      "Train Epoch: 9 [2400/2743 (86%)]\tLoss: 0.265560\n",
      "\n",
      "Test set: Average loss: 0.3868, Accuracy: 283/343 (83%)\n",
      ", Recall: 0.8445860391527145, Precision: 0.8432753706536983\n",
      "\n",
      "Train Epoch: 10 [0/2743 (0%)]\tLoss: 0.232856\n",
      "Train Epoch: 10 [400/2743 (14%)]\tLoss: 0.229462\n",
      "Train Epoch: 10 [800/2743 (29%)]\tLoss: 0.238298\n",
      "Train Epoch: 10 [1200/2743 (43%)]\tLoss: 0.222049\n",
      "Train Epoch: 10 [1600/2743 (57%)]\tLoss: 0.240755\n",
      "Train Epoch: 10 [2000/2743 (71%)]\tLoss: 0.240379\n",
      "Train Epoch: 10 [2400/2743 (86%)]\tLoss: 0.243887\n",
      "\n",
      "Test set: Average loss: 0.3772, Accuracy: 283/343 (83%)\n",
      ", Recall: 0.848708271462355, Precision: 0.8412675935050108\n",
      "\n",
      "Train Epoch: 11 [0/2743 (0%)]\tLoss: 0.204965\n",
      "Train Epoch: 11 [400/2743 (14%)]\tLoss: 0.226380\n",
      "Train Epoch: 11 [800/2743 (29%)]\tLoss: 0.193027\n",
      "Train Epoch: 11 [1200/2743 (43%)]\tLoss: 0.190607\n",
      "Train Epoch: 11 [1600/2743 (57%)]\tLoss: 0.200001\n",
      "Train Epoch: 11 [2000/2743 (71%)]\tLoss: 0.246562\n",
      "Train Epoch: 11 [2400/2743 (86%)]\tLoss: 0.208287\n",
      "\n",
      "Test set: Average loss: 0.3742, Accuracy: 282/343 (82%)\n",
      ", Recall: 0.8377872049505177, Precision: 0.8394969700931939\n",
      "\n",
      "Train Epoch: 12 [0/2743 (0%)]\tLoss: 0.220897\n",
      "Train Epoch: 12 [400/2743 (14%)]\tLoss: 0.218325\n",
      "Train Epoch: 12 [800/2743 (29%)]\tLoss: 0.183038\n",
      "Train Epoch: 12 [1200/2743 (43%)]\tLoss: 0.192941\n",
      "Train Epoch: 12 [1600/2743 (57%)]\tLoss: 0.198602\n",
      "Train Epoch: 12 [2000/2743 (71%)]\tLoss: 0.197991\n",
      "Train Epoch: 12 [2400/2743 (86%)]\tLoss: 0.215065\n",
      "\n",
      "Test set: Average loss: 0.3709, Accuracy: 287/343 (84%)\n",
      ", Recall: 0.8520072020968035, Precision: 0.8520918335535761\n",
      "\n",
      "Train Epoch: 13 [0/2743 (0%)]\tLoss: 0.185742\n",
      "Train Epoch: 13 [400/2743 (14%)]\tLoss: 0.172724\n",
      "Train Epoch: 13 [800/2743 (29%)]\tLoss: 0.225275\n",
      "Train Epoch: 13 [1200/2743 (43%)]\tLoss: 0.202928\n",
      "Train Epoch: 13 [1600/2743 (57%)]\tLoss: 0.181184\n",
      "Train Epoch: 13 [2000/2743 (71%)]\tLoss: 0.170857\n",
      "Train Epoch: 13 [2400/2743 (86%)]\tLoss: 0.213345\n",
      "\n",
      "Test set: Average loss: 0.3685, Accuracy: 288/343 (84%)\n",
      ", Recall: 0.8588057617774709, Precision: 0.8547076746526706\n",
      "\n",
      "Train Epoch: 14 [0/2743 (0%)]\tLoss: 0.179672\n",
      "Train Epoch: 14 [400/2743 (14%)]\tLoss: 0.169871\n",
      "Train Epoch: 14 [800/2743 (29%)]\tLoss: 0.187592\n",
      "Train Epoch: 14 [1200/2743 (43%)]\tLoss: 0.212117\n",
      "Train Epoch: 14 [1600/2743 (57%)]\tLoss: 0.160721\n",
      "Train Epoch: 14 [2000/2743 (71%)]\tLoss: 0.203926\n",
      "Train Epoch: 14 [2400/2743 (86%)]\tLoss: 0.207662\n",
      "\n",
      "Test set: Average loss: 0.3666, Accuracy: 290/343 (85%)\n",
      ", Recall: 0.8640399439902945, Precision: 0.8582578847671467\n",
      "\n",
      "Train Epoch: 15 [0/2743 (0%)]\tLoss: 0.162002\n",
      "Train Epoch: 15 [400/2743 (14%)]\tLoss: 0.169671\n",
      "Train Epoch: 15 [800/2743 (29%)]\tLoss: 0.175796\n",
      "Train Epoch: 15 [1200/2743 (43%)]\tLoss: 0.208541\n",
      "Train Epoch: 15 [1600/2743 (57%)]\tLoss: 0.153924\n",
      "Train Epoch: 15 [2000/2743 (71%)]\tLoss: 0.187102\n",
      "Train Epoch: 15 [2400/2743 (86%)]\tLoss: 0.186808\n",
      "\n",
      "Test set: Average loss: 0.3672, Accuracy: 284/343 (83%)\n",
      ", Recall: 0.8474610612754943, Precision: 0.8430942374845924\n",
      "\n",
      "Train Epoch: 16 [0/2743 (0%)]\tLoss: 0.180193\n",
      "Train Epoch: 16 [400/2743 (14%)]\tLoss: 0.193993\n",
      "Train Epoch: 16 [800/2743 (29%)]\tLoss: 0.213146\n",
      "Train Epoch: 16 [1200/2743 (43%)]\tLoss: 0.148726\n",
      "Train Epoch: 16 [1600/2743 (57%)]\tLoss: 0.177876\n",
      "Train Epoch: 16 [2000/2743 (71%)]\tLoss: 0.186343\n",
      "Train Epoch: 16 [2400/2743 (86%)]\tLoss: 0.185492\n",
      "\n",
      "Test set: Average loss: 0.3648, Accuracy: 285/343 (83%)\n",
      ", Recall: 0.8479629890233606, Precision: 0.8453712868290024\n",
      "\n",
      "Train Epoch: 17 [0/2743 (0%)]\tLoss: 0.159491\n",
      "Train Epoch: 17 [400/2743 (14%)]\tLoss: 0.171313\n",
      "Train Epoch: 17 [800/2743 (29%)]\tLoss: 0.215922\n",
      "Train Epoch: 17 [1200/2743 (43%)]\tLoss: 0.162616\n",
      "Train Epoch: 17 [1600/2743 (57%)]\tLoss: 0.160036\n",
      "Train Epoch: 17 [2000/2743 (71%)]\tLoss: 0.164718\n",
      "Train Epoch: 17 [2400/2743 (86%)]\tLoss: 0.170591\n",
      "\n",
      "Test set: Average loss: 0.3656, Accuracy: 287/343 (84%)\n",
      ", Recall: 0.8545376647162362, Precision: 0.8505259260042601\n",
      "\n",
      "Train Epoch: 18 [0/2743 (0%)]\tLoss: 0.191618\n",
      "Train Epoch: 18 [400/2743 (14%)]\tLoss: 0.198116\n",
      "Train Epoch: 18 [800/2743 (29%)]\tLoss: 0.173038\n",
      "Train Epoch: 18 [1200/2743 (43%)]\tLoss: 0.201308\n",
      "Train Epoch: 18 [1600/2743 (57%)]\tLoss: 0.184311\n",
      "Train Epoch: 18 [2000/2743 (71%)]\tLoss: 0.151181\n",
      "Train Epoch: 18 [2400/2743 (86%)]\tLoss: 0.137762\n",
      "\n",
      "Test set: Average loss: 0.3637, Accuracy: 285/343 (83%)\n",
      ", Recall: 0.8487866510057028, Precision: 0.845813156428374\n",
      "\n",
      "Train Epoch: 19 [0/2743 (0%)]\tLoss: 0.156979\n",
      "Train Epoch: 19 [400/2743 (14%)]\tLoss: 0.161838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [800/2743 (29%)]\tLoss: 0.177044\n",
      "Train Epoch: 19 [1200/2743 (43%)]\tLoss: 0.185929\n",
      "Train Epoch: 19 [1600/2743 (57%)]\tLoss: 0.170164\n",
      "Train Epoch: 19 [2000/2743 (71%)]\tLoss: 0.206910\n",
      "Train Epoch: 19 [2400/2743 (86%)]\tLoss: 0.186873\n",
      "\n",
      "Test set: Average loss: 0.3671, Accuracy: 288/343 (84%)\n",
      ", Recall: 0.8563544325134778, Precision: 0.8542572692878422\n",
      "\n",
      "Train Epoch: 20 [0/2743 (0%)]\tLoss: 0.154609\n",
      "Train Epoch: 20 [400/2743 (14%)]\tLoss: 0.191709\n",
      "Train Epoch: 20 [800/2743 (29%)]\tLoss: 0.153281\n",
      "Train Epoch: 20 [1200/2743 (43%)]\tLoss: 0.190642\n",
      "Train Epoch: 20 [1600/2743 (57%)]\tLoss: 0.146681\n",
      "Train Epoch: 20 [2000/2743 (71%)]\tLoss: 0.175160\n",
      "Train Epoch: 20 [2400/2743 (86%)]\tLoss: 0.174964\n",
      "\n",
      "Test set: Average loss: 0.3669, Accuracy: 286/343 (83%)\n",
      ", Recall: 0.8507798876902616, Precision: 0.8491026301125845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelNonVpnRnnBinary = RNNMulti().to(device)\n",
    "\n",
    "args = {'lr': 0.5,\n",
    "              'gamma': 0.7,\n",
    "              'dry-run': False,\n",
    "              'log_interval': 2,\n",
    "              'epochs': 20\n",
    "             }\n",
    "\n",
    "optimizer = torch.optim.Adadelta(modelNonVpnRnnBinary.parameters(), lr=args['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args['gamma'])\n",
    "\n",
    "test_result = pd.DataFrame(columns=['Epoch','Loss','Accuracy'])\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train(args, modelNonVpnRnnBinary, device, nonVpnTrainLoader, optimizer, epoch)\n",
    "    (loss, accuracy) = test(modelNonVpnRnnBinary, device, nonVpnValidateLoader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3470, Accuracy: 298/343 (87%)\n",
      ", Recall: 0.8782962803322586, Precision: 0.8695638307480413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(loss, accuracy) = test(modelNonVpnRnnBinary, device, nonVpnTestLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Final result evaluation\n",
    "\n",
    "We have done three expriment:\n",
    "1. Binary classfication on VPN and non-VPN traffic\n",
    "2. OpenVPN traffic classfication on 4 classes\n",
    "3. Regular encryted traffic (TLS,HTTPS) classfication on 4 classes\n",
    "\n",
    "### Experiment 1\n",
    "\n",
    "In this experiment, we re-implemented previous work on 1-D CNN (Wang, Wei, et al. 2017) and compared to RNN(LSTM) model, there are no slight difference on performance.\n",
    "\n",
    "### Experiment 2\n",
    "\n",
    "In this experiment, RNN(LSTM) model is used to identify traffic under OpenVPN encryption, the performance on recall and precision is worse than 1-D CNN. However, the accuracy itself is acceptable.\n",
    "\n",
    "### Experiment3 \n",
    "\n",
    "In this experiment, RNN(LSTM) model is "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
